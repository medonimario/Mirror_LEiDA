{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "##############################################################################\n",
    "# 1) Load data into a nested dictionary\n",
    "##############################################################################\n",
    "def load_leading_eigenvectors(data_dir):\n",
    "    \"\"\"\n",
    "    Loads .npy files of shape (n_epochs, n_windows, n_channels) from data_dir.\n",
    "    Files are named like 's_{subject_number}_{Condition}-eigenvectors.npy'.\n",
    "    \n",
    "    Returns a nested dict with this structure:\n",
    "        data_dict[condition][subject_number] = eigenvectors, shape (n_epochs, n_windows, n_channels)\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Example conditions you expect to find in filenames\n",
    "    possible_conditions = [\"Coordination\", \"Solo\", \"Spontaneous\"]\n",
    "    \n",
    "    for fname in sorted(os.listdir(data_dir)):\n",
    "\n",
    "        if fname.endswith(\".npy\") and \"eigenvectors\" in fname:\n",
    "            # Strip extension\n",
    "            base = fname.replace(\".npy\",\"\")\n",
    "            \n",
    "            # Remove any \"s_\" prefix if it exists\n",
    "            if base.startswith(\"s_\"):\n",
    "                base = base[2:]\n",
    "\n",
    "            parts = base.split(\"_\", 1)\n",
    "            subject_str = parts[0]\n",
    "            \n",
    "            remainder = parts[1]\n",
    "\n",
    "            cond_part = remainder.split(\"-\")[0]\n",
    "\n",
    "            condition = None\n",
    "            for c in possible_conditions:\n",
    "                if cond_part.lower().startswith(c.lower()):\n",
    "                    condition = c\n",
    "                    break\n",
    "            if condition is None:\n",
    "                print(f\"Warning: condition not recognized in filename {fname}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Load file\n",
    "            filepath = os.path.join(data_dir, fname)\n",
    "            data_array = np.load(filepath)\n",
    "            \n",
    "            # Initialize sub-dict if needed\n",
    "            if condition not in data_dict:\n",
    "                data_dict[condition] = {}\n",
    "            \n",
    "            data_dict[condition][subject_str] = data_array\n",
    "            print(f\"Loaded file {fname} for subject={subject_str}, condition={condition}, shape={data_array.shape}\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "##############################################################################\n",
    "# 2) Collate data into a 2D array (and keep track of indices)\n",
    "##############################################################################\n",
    "def collate_eigenvectors(data_dict):\n",
    "    \"\"\"\n",
    "    Takes the nested data_dict[condition][subject] = array of shape \n",
    "    (n_epochs, n_windows, n_channels)\n",
    "    and stacks all conditions & subjects into one 2D array of shape:\n",
    "       (total_epochs * total_windows, n_channels)\n",
    "\n",
    "    Also builds a 'meta' list describing how each row in the flattened array\n",
    "    maps back to (condition, subject, epoch_idx, window_idx).\n",
    "\n",
    "    Returns: coll_eigs, meta_list\n",
    "    \"\"\"\n",
    "    coll_list = []\n",
    "    meta_list = []\n",
    "    \n",
    "    for condition, subj_dict in data_dict.items():\n",
    "        for subject, eigenvectors in subj_dict.items():\n",
    "            n_epochs, n_windows, n_channels = eigenvectors.shape\n",
    "            \n",
    "            # For each epoch and window, we have one row\n",
    "            # So we'll reshape (n_epochs, n_windows, n_channels) -> (n_epochs*n_windows, n_channels)\n",
    "            reshaped = eigenvectors.reshape(-1, n_channels)\n",
    "            \n",
    "            coll_list.append(reshaped)\n",
    "            \n",
    "            # Build meta info for each row in 'reshaped'\n",
    "            # We need (condition, subject, epoch_idx, window_idx)\n",
    "            for e in range(n_epochs):\n",
    "                for w in range(n_windows):\n",
    "                    meta_list.append((condition, subject, e, w))\n",
    "    \n",
    "    # Stack vertically\n",
    "    coll_eigs = np.vstack(coll_list)\n",
    "    coll_eigs_shape = coll_eigs.shape\n",
    "    print(f\"Collated shape = {coll_eigs_shape} (rows, n_channels).  #rows = sum of (n_epochs*n_windows) across all data.\")\n",
    "    \n",
    "    return coll_eigs, meta_list\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 3) K-means function (similar to run_leida_kmeans)\n",
    "##############################################################################\n",
    "def run_leida_kmeans(coll_eigenvectors, K, n_init=50, max_iter=200, random_state=None):\n",
    "    \"\"\"\n",
    "    Replicates the essential steps of LEiDA_EEG_kmeans in Python:\n",
    "      - K-means with 'sqeuclidean' distance \n",
    "      - Re-label clusters by descending frequency (largest cluster -> cluster 0, etc.).\n",
    "    Returns dict with keys ['IDX', 'C', 'counts', 'distances'].\n",
    "    \"\"\"\n",
    "    print(f\"Running k-means with K={K}, n_init={n_init}, max_iter={max_iter}\")\n",
    "    kmeans = KMeans(n_clusters=K,\n",
    "                    n_init=n_init,\n",
    "                    max_iter=max_iter,\n",
    "                    random_state=random_state,\n",
    "                    verbose=0)\n",
    "    kmeans.fit(coll_eigenvectors)\n",
    "    \n",
    "    old_labels = kmeans.labels_        # shape [n_samples,]\n",
    "    centers   = kmeans.cluster_centers_\n",
    "    # Sort by descending size\n",
    "    counts = np.bincount(old_labels, minlength=K)\n",
    "    ind_sort = np.argsort(counts)[::-1]  # largest cluster first\n",
    "    \n",
    "    # Create cluster_map: old_label -> new_label\n",
    "    cluster_map = np.zeros(K, dtype=int)\n",
    "    for new_label, old_label in enumerate(ind_sort):\n",
    "        cluster_map[old_label] = new_label\n",
    "    \n",
    "    new_labels = cluster_map[old_labels]\n",
    "    new_centers = centers[ind_sort, :]\n",
    "    distances = kmeans.transform(coll_eigenvectors)\n",
    "    \n",
    "    return {\n",
    "        'IDX': new_labels,           \n",
    "        'C': new_centers,           \n",
    "        'counts': counts[ind_sort],  \n",
    "        'distances': distances,      \n",
    "    }\n",
    "\n",
    "##############################################################################\n",
    "# 4) Map cluster labels back into the same dictionary structure\n",
    "##############################################################################\n",
    "def map_labels_back(kmeans_labels, meta_list, data_dict):\n",
    "    \"\"\"\n",
    "    Reorganize the 1D array of cluster assignments (kmeans_labels) into\n",
    "    a parallel nested dictionary matching data_dict’s keys and shapes.\n",
    "\n",
    "    data_dict[condition][subject] has shape (n_epochs, n_windows, n_channels),\n",
    "    so we create labels_dict[condition][subject], shape = (n_epochs, n_windows).\n",
    "    \"\"\"\n",
    "    # Prepare an empty structure\n",
    "    labels_dict = {}\n",
    "    \n",
    "    # We need to know how many windows per row. But we can reconstruct from meta_list:\n",
    "    # meta_list[i] = (condition, subject, epoch_idx, window_idx)\n",
    "    # We'll gather the assigned cluster for each row.\n",
    "    \n",
    "    for (label_value, meta) in zip(kmeans_labels, meta_list):\n",
    "        (condition, subject, e, w) = meta\n",
    "        # Initialize empty dictionary if needed\n",
    "        if condition not in labels_dict:\n",
    "            labels_dict[condition] = {}\n",
    "        if subject not in labels_dict[condition]:\n",
    "            # figure out shape from data_dict to pre-allocate\n",
    "            n_epochs, n_windows, _ = data_dict[condition][subject].shape\n",
    "            labels_dict[condition][subject] = np.zeros((n_epochs, n_windows), dtype=int)\n",
    "        \n",
    "        labels_dict[condition][subject][e, w] = label_value\n",
    "    \n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/leading/MNE/alpha/\"\n",
    "\n",
    "# 1) Load data into dictionary\n",
    "data_dict = load_leading_eigenvectors(data_dir)\n",
    "print(f\"Loaded data_dict with {len(data_dict)} conditions and {sum(len(v) for v in data_dict.values())} subjects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Collate all eigenvectors\n",
    "coll_eigs, meta_list = collate_eigenvectors(data_dict)\n",
    "\n",
    "# 3) Run k-means \n",
    "k = 10\n",
    "kmeans_results = run_leida_kmeans(coll_eigs, K=k, n_init=50, max_iter=100, random_state=42)\n",
    "\n",
    "# 4) Map cluster labels back\n",
    "labels_dict = map_labels_back(kmeans_results['IDX'], meta_list, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an example\n",
    "for cond in labels_dict:\n",
    "    for subj in labels_dict[cond]:\n",
    "        print(f\"{cond}, subject={subj}, labels shape = {labels_dict[cond][subj].shape}\")\n",
    "        # Check cluster label of the first epoch-window\n",
    "        # print(labels_dict[cond][subj][0,0])\n",
    "\n",
    "# And you can also access cluster centers from kmeans_res['C']\n",
    "print(f\"Cluster centers shape: {kmeans_results['C'].shape}\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .npy files\n",
    "data_dir = '../data/leading/MNE/alpha/'\n",
    "# go through all files in the directory and add them to a list\n",
    "recordings = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        #load the .npy file\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        # load the .npy file\n",
    "        data = np.load(filepath)\n",
    "        print(f'Loaded {filename} with shape {data.shape}') # shape should be (n_epochs, n_windows, n_channels)\n",
    "        # append the data to the list\n",
    "        recordings.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loaded {len(recordings)} recordings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the recordings along the first axis\n",
    "coll_eigenvectors = np.vstack(recordings)\n",
    "print(f'Collated eigenvectors shape: {coll_eigenvectors.shape}') # shape should be (n_recordings * n_epochs, n_windows, n_channels)\n",
    "# reshape the data to (n_recordings * n_epochs * n_windows, n_channels)\n",
    "coll_eigenvectors = coll_eigenvectors.reshape(-1, coll_eigenvectors.shape[-1])\n",
    "print(f'Collated eigenvectors reshaped to: {coll_eigenvectors.shape}') # shape should be (n_recordings * n_epochs * n_windows, n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run k-means for a chosen k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_leida_kmeans(coll_eigenvectors, K, n_init=50, max_iter=200, random_state=None):\n",
    "    \"\"\"\n",
    "    Replicates the essential steps of LEiDA_EEG_kmeans.m in Python:\n",
    "      - K-means with 'sqeuclidean' (which is the usual sum-of-squares in scikit-learn).\n",
    "      - Re-label clusters by size (largest cluster -> cluster #0, second largest -> #1, etc.).\n",
    "    Returns a dict similar to the MATLAB struct.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Run k-means with the given K\n",
    "    #    'algorithm'='lloyd' is standard, inertia_ is sum of squared distances\n",
    "    kmeans = KMeans(n_clusters=K,\n",
    "                    n_init=n_init,\n",
    "                    max_iter=max_iter,\n",
    "                    random_state=random_state,\n",
    "                    verbose=1\n",
    "                   )\n",
    "    kmeans.fit(coll_eigenvectors)\n",
    "\n",
    "    # labels_: shape [n_samples,], each sample assigned cluster 0..K-1\n",
    "    old_labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 2) Sort clusters by descending frequency (size)\n",
    "    #    Count how many rows ended up in each cluster\n",
    "    counts = np.bincount(old_labels, minlength=K)  # shape [K]\n",
    "    # Sort them descending by cluster size\n",
    "    ind_sort = np.argsort(counts)[::-1]  # largest cluster first\n",
    "    # Create a mapping array: cluster 'ind_sort[0]' -> new label 0, etc.\n",
    "    cluster_map = np.zeros(K, dtype=int)\n",
    "    for new_label, old_label in enumerate(ind_sort):\n",
    "        cluster_map[old_label] = new_label\n",
    "\n",
    "    # 3) Re-label the assignments according to cluster size\n",
    "    new_labels = cluster_map[old_labels]\n",
    "\n",
    "    # 4) Reorder the cluster centers the same way\n",
    "    new_centers = centers[ind_sort, :]\n",
    "    # 5) Reorder the distances to the new centers\n",
    "    distances = kmeans.transform(coll_eigenvectors)\n",
    "\n",
    "    # Return results in a dictionary (mimicking MATLAB struct)\n",
    "    return {\n",
    "        'IDX': new_labels,           # cluster labels (time course) after re-labeling\n",
    "        'C': new_centers,            # cluster centroids\n",
    "        'counts': counts[ind_sort],  # cluster sizes in descending order\n",
    "        'distances': distances,      # distance of each point to each centroid\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means\n",
    "k = 10\n",
    "kmeans_results = run_leida_kmeans(coll_eigenvectors, k, n_init=50, max_iter=100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_kmeans_3d_pca(collEigenvectors, labels, centers, title=\"K-means Clusters in PCA Space\", max_points=1000):\n",
    "    \"\"\"\n",
    "    Project eigenvector data (collEigenvectors) onto 3 principal components,\n",
    "    then scatter-plot them in 3D, color-coded by cluster label. The cluster\n",
    "    centers are also projected and plotted as black-edged triangles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    collEigenvectors : ndarray, shape (n_samples, n_features)\n",
    "        All collated eigenvectors that were clustered.\n",
    "    labels : ndarray, shape (n_samples,)\n",
    "        Integer cluster labels for each row in collEigenvectors.\n",
    "    centers : ndarray, shape (K, n_features)\n",
    "        The cluster centroids (in original feature space).\n",
    "    title : str\n",
    "        Title for the plot (optional).\n",
    "    max_points : int\n",
    "        Maximum number of points to plot (will subsample if exceeded)\n",
    "    \"\"\"\n",
    "    # Subsample data if too many points\n",
    "    n_samples = collEigenvectors.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    if n_samples > max_points:\n",
    "        print(f\"Subsampling {max_points} points from {n_samples} total points for visualization\")\n",
    "        # Stratified sampling to maintain cluster proportions\n",
    "        sampled_indices = []\n",
    "        for k in np.unique(labels):\n",
    "            cluster_indices = indices[labels == k]\n",
    "            # Calculate proportional sample size for this cluster\n",
    "            cluster_size = len(cluster_indices)\n",
    "            sample_size = max(1, int(cluster_size * max_points / n_samples))\n",
    "            # Sample indices from this cluster\n",
    "            if sample_size < cluster_size:\n",
    "                sampled_cluster_indices = np.random.choice(cluster_indices, size=sample_size, replace=False)\n",
    "            else:\n",
    "                sampled_cluster_indices = cluster_indices\n",
    "            sampled_indices.extend(sampled_cluster_indices)\n",
    "        \n",
    "        # Convert to numpy array and shuffle\n",
    "        sampled_indices = np.array(sampled_indices)\n",
    "        np.random.shuffle(sampled_indices)\n",
    "        \n",
    "        # Limit to max_points (in case rounding caused more points)\n",
    "        if len(sampled_indices) > max_points:\n",
    "            sampled_indices = sampled_indices[:max_points]\n",
    "            \n",
    "        # Subsample the data and labels\n",
    "        plot_vectors = collEigenvectors[sampled_indices]\n",
    "        plot_labels = labels[sampled_indices]\n",
    "    else:\n",
    "        plot_vectors = collEigenvectors\n",
    "        plot_labels = labels\n",
    "    \n",
    "    # 1) Run PCA to reduce dimensionality to 3\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(plot_vectors)     # shape: [sampled_points, 3]\n",
    "    centers_pca = pca.transform(centers)        # shape: [K, 3]\n",
    "    \n",
    "    # 2) Plot the data, color-coded by cluster label\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        X_pca[:, 0], \n",
    "        X_pca[:, 1], \n",
    "        X_pca[:, 2],\n",
    "        c=plot_labels,                # use cluster labels as color\n",
    "        cmap='rainbow',\n",
    "        alpha=0.6,\n",
    "        s=30                         # slightly larger points for better visibility\n",
    "    )\n",
    "    \n",
    "    # 3) Plot the cluster centers as bigger, distinct markers\n",
    "    K = centers.shape[0]\n",
    "    ax.scatter(\n",
    "        centers_pca[:, 0], \n",
    "        centers_pca[:, 1], \n",
    "        centers_pca[:, 2],\n",
    "        c=np.arange(K),         # color them using the same colormap but indexed by cluster\n",
    "        cmap='rainbow',\n",
    "        marker='^',\n",
    "        s=200,\n",
    "        edgecolors='k',\n",
    "        linewidths=1.5,\n",
    "        label='Cluster centers'\n",
    "    )\n",
    "    \n",
    "    # 4) Tidy up the figure\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add explained variance information\n",
    "    explained_var = pca.explained_variance_ratio_ * 100\n",
    "    ax.text2D(0.02, 0.95, f\"Explained variance: PC1={explained_var[0]:.1f}%, PC2={explained_var[1]:.1f}%, PC3={explained_var[2]:.1f}%\", \n",
    "             transform=ax.transAxes)\n",
    "    \n",
    "    # Add info about subsampling if applicable\n",
    "    if n_samples > max_points:\n",
    "        ax.text2D(0.02, 0.90, f\"Showing {max_points} of {n_samples} points\", transform=ax.transAxes)\n",
    "    \n",
    "    # Colorbar with cluster label indices\n",
    "    cbar = fig.colorbar(scatter, ax=ax, fraction=0.03, pad=0.07)\n",
    "    cbar.set_label(\"Cluster Label\")\n",
    "    \n",
    "    # Add a legend\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    # Add grid lines for better 3D perception\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the k-means results\n",
    "plot_kmeans_3d_pca(\n",
    "    coll_eigs, \n",
    "    kmeans_results['IDX'], \n",
    "    kmeans_results['C'], \n",
    "    title=f\"K-means Clusters in PCA Space (K={k})\",\n",
    "    max_points=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for K={k}:\\n\", \n",
    "      f\"Cluster sizes: {kmeans_results['counts']}\\n\",\n",
    "      f\"Cluster centers:\\n{kmeans_results['C'][:k]}...\\n\", \n",
    "      f\"Distances:\\n{kmeans_results['distances'][:k]}...\\n\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = mne.read_epochs(\"../data/source/s_101_Coordination-source-epo.fif\")\n",
    "roi_names = epochs.ch_names  # Each name is the label of a parcellated ROI\n",
    "print(\"ROI names:\", roi_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=k, ncols=1, figsize=(12, 3*k), sharex=True)\n",
    "if k == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(k):\n",
    "    ax = axes[i]\n",
    "    center_vec = kmeans_results['C'][i, :]\n",
    "    \n",
    "    # Color bars red if positive, blue if negative\n",
    "    colors = ['red' if val >= 0 else 'blue' for val in center_vec]\n",
    "    \n",
    "    ax.bar(\n",
    "        x=range(len(center_vec)),\n",
    "        height=center_vec,\n",
    "        color=colors\n",
    "    )\n",
    "    ax.axhline(0, color='black', linewidth=1)  # horizontal line at 0\n",
    "    \n",
    "    ax.set_title(f\"Cluster {i} center\", fontsize=12)\n",
    "    ax.set_ylabel(\"Eigenvector component\")\n",
    "    # Label the x-ticks with ROI names, rotate if needed\n",
    "    ax.set_xticks(range(len(center_vec)))\n",
    "    ax.set_xticklabels(roi_names, rotation=90, fontsize=9)\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that groups the Desikan–Killiany atlas ROIs into lobes/regions\n",
    "lobe_groups = {\n",
    "    \"Frontal\": [\n",
    "        \"caudalmiddlefrontal-lh\", \"caudalmiddlefrontal-rh\",\n",
    "        \"lateralorbitofrontal-lh\", \"lateralorbitofrontal-rh\",\n",
    "        \"medialorbitofrontal-lh\", \"medialorbitofrontal-rh\",\n",
    "        \"parsopercularis-lh\", \"parsopercularis-rh\",\n",
    "        \"parsorbitalis-lh\", \"parsorbitalis-rh\",\n",
    "        \"parstriangularis-lh\", \"parstriangularis-rh\",\n",
    "        \"precentral-lh\", \"precentral-rh\",\n",
    "        \"rostralmiddlefrontal-lh\", \"rostralmiddlefrontal-rh\",\n",
    "        \"superiorfrontal-lh\", \"superiorfrontal-rh\",\n",
    "        \"frontalpole-lh\", \"frontalpole-rh\"\n",
    "    ],\n",
    "    \"Parietal\": [\n",
    "        \"inferiorparietal-lh\", \"inferiorparietal-rh\",\n",
    "        \"superiorparietal-lh\", \"superiorparietal-rh\",\n",
    "        \"supramarginal-lh\", \"supramarginal-rh\",\n",
    "        \"postcentral-lh\", \"postcentral-rh\",\n",
    "        \"precuneus-lh\", \"precuneus-rh\",\n",
    "        \"paracentral-lh\", \"paracentral-rh\"  # Often near boundary of frontal/parietal\n",
    "    ],\n",
    "    \"Temporal\": [\n",
    "        \"bankssts-lh\", \"bankssts-rh\",\n",
    "        \"entorhinal-lh\", \"entorhinal-rh\",\n",
    "        \"fusiform-lh\", \"fusiform-rh\",\n",
    "        \"inferiortemporal-lh\", \"inferiortemporal-rh\",\n",
    "        \"middletemporal-lh\", \"middletemporal-rh\",\n",
    "        \"parahippocampal-lh\", \"parahippocampal-rh\",\n",
    "        \"superiortemporal-lh\", \"superiortemporal-rh\",\n",
    "        \"temporalpole-lh\", \"temporalpole-rh\",\n",
    "        \"transversetemporal-lh\", \"transversetemporal-rh\"\n",
    "    ],\n",
    "    \"Occipital\": [\n",
    "        \"cuneus-lh\", \"cuneus-rh\",\n",
    "        \"lateraloccipital-lh\", \"lateraloccipital-rh\",\n",
    "        \"lingual-lh\", \"lingual-rh\",\n",
    "        \"pericalcarine-lh\", \"pericalcarine-rh\"\n",
    "    ],\n",
    "    \"Cingulate\": [\n",
    "        \"caudalanteriorcingulate-lh\", \"caudalanteriorcingulate-rh\",\n",
    "        \"isthmuscingulate-lh\", \"isthmuscingulate-rh\",\n",
    "        \"posteriorcingulate-lh\", \"posteriorcingulate-rh\",\n",
    "        \"rostralanteriorcingulate-lh\", \"rostralanteriorcingulate-rh\"\n",
    "    ],\n",
    "    \"Insula\": [\n",
    "        \"insula-lh\", \"insula-rh\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build a lookup: ROI name -> original index\n",
    "roi_to_index = {roi: idx for idx, roi in enumerate(roi_names)}\n",
    "\n",
    "# Define an *ordered* list of lobes for plotting,\n",
    "# so we can control which lobe goes first, second, etc.\n",
    "ordered_lobes = [\"Frontal\", \"Parietal\", \"Temporal\", \"Occipital\", \"Cingulate\", \"Insula\"]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Build a new ROI order that groups lobes consecutively\n",
    "#    Also track the group boundaries so we know the start/end index\n",
    "# ---------------------------------------------------------------------------\n",
    "roi_names_ordered = []\n",
    "group_boundaries = []  # list of (group_name, start_idx, end_idx)\n",
    "\n",
    "start_idx = 0\n",
    "for lobe_name in ordered_lobes:\n",
    "    # All ROIs in this group that actually appear in roi_names\n",
    "    rois_in_this_group = [r for r in lobe_groups[lobe_name] if r in roi_to_index]\n",
    "    if not rois_in_this_group:\n",
    "        # skip if no ROI is found for this group\n",
    "        continue\n",
    "\n",
    "    for roi in rois_in_this_group:\n",
    "        roi_names_ordered.append(roi)\n",
    "\n",
    "    end_idx = len(roi_names_ordered) - 1\n",
    "    group_boundaries.append((lobe_name, start_idx, end_idx))\n",
    "\n",
    "    start_idx = end_idx + 1\n",
    "\n",
    "# Build an index array so we can reorder each cluster center\n",
    "plot_indices = [roi_to_index[r] for r in roi_names_ordered]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2) Plot each cluster, reordering the ROI axis so that same-lobe ROIs are consecutive\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(nrows=k, ncols=1, figsize=(12, 3*k))\n",
    "if k == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for cluster_i, ax in enumerate(axes):\n",
    "    # Original center vector\n",
    "    center_vec = kmeans_results['C'][cluster_i, :]\n",
    "\n",
    "    # Reorder to match our new grouping\n",
    "    center_vec_ordered = center_vec[plot_indices]\n",
    "\n",
    "    # Bar colors: red if >= 0, else blue\n",
    "    bar_colors = ['red' if val >= 0 else 'blue' for val in center_vec_ordered]\n",
    "\n",
    "    # Plot\n",
    "    ax.bar(x=range(len(center_vec_ordered)),\n",
    "           height=center_vec_ordered,\n",
    "           color=bar_colors,\n",
    "           alpha=0.7)\n",
    "    ax.axhline(0, color='black', linewidth=1)\n",
    "\n",
    "    ax.set_title(f\"Cluster {cluster_i} center\", fontsize=12)\n",
    "    ax.set_ylabel(\"Eigenvector\\ncomponent\")\n",
    "    ax.set_xticks(range(len(center_vec_ordered)))\n",
    "    ax.set_xticklabels(roi_names_ordered, rotation=90, fontsize=8)\n",
    "\n",
    "    # Color tick labels\n",
    "    for idx, tick_label in enumerate(ax.get_xticklabels()):\n",
    "        val = center_vec_ordered[idx]\n",
    "        tick_label.set_color('red' if val >= 0 else 'blue')\n",
    "\n",
    "    # Now shade groups. Because we've reordered, each lobe's ROIs are consecutive\n",
    "    # We'll label the group name near the top\n",
    "    ymax = max(center_vec_ordered.max(), 0)\n",
    "    y_label_pos = 1.07 * ymax if ymax != 0 else 0.05  # if all negative, place label near 0\n",
    "\n",
    "    for lobe_name, start_idx, end_idx in group_boundaries:\n",
    "        ax.axvspan(start_idx - 0.4, end_idx + 0.4, alpha=0.12, color='gray')\n",
    "        mid = 0.5 * (start_idx + end_idx)\n",
    "        ax.text(mid, y_label_pos, lobe_name,\n",
    "                ha='center', va='bottom', fontsize=9, color='black')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS_DIR = \"~/mne_data/MNE-fsaverage-data/\" \n",
    "labels = mne.read_labels_from_annot(\n",
    "    subject=\"fsaverage\",\n",
    "    parc=\"aparc\",\n",
    "    subjects_dir=SUBJECTS_DIR\n",
    ")\n",
    "labels = labels[:-1]\n",
    "\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain = Brain(\n",
    "    subject=\"fsaverage\",\n",
    "    hemi=\"both\",\n",
    "    surf=\"white\",\n",
    "    subjects_dir=SUBJECTS_DIR,\n",
    "    background=\"white\",\n",
    "    size=(800, 600),\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "cluster_i = 0\n",
    "cluster_vec = kmeans_results['C'][cluster_i, :]\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    # Get the color for this cluster\n",
    "    val = cluster_vec[i]\n",
    "    if val > 0:\n",
    "        color = 'red'\n",
    "        # Set the color for this label\n",
    "        label.color = color\n",
    "        # Add the label to the brain\n",
    "        brain.add_label(label, hemi=\"both\", color=color, alpha=0.7)\n",
    "        brain.add_label(label, hemi=\"both\", color=\"black\", alpha=0.8, borders=1)\n",
    "\n",
    "# Show the brain\n",
    "brain.show_view(\"lateral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import numpy as np\n",
    "import mne\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def visualize_brain_clusters(k, kmeans_results, roi_names, labels, \n",
    "                             view_angles=['lateral-left', 'lateral-right', 'dorsal', 'ventral'],\n",
    "                             subjects_dir=\"~/mne_data/MNE-fsaverage-data/\"):\n",
    "    \"\"\"\n",
    "    Visualize brain clusters from k-means solutions by highlighting opposite-phase ROIs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    k : int\n",
    "        The number of clusters in the k-means solution to visualize\n",
    "    kmeans_results : dict\n",
    "        Dictionary containing k-means results, including 'C' for cluster centers\n",
    "    roi_names : list\n",
    "        List of ROI names corresponding to the dimensions of each cluster center\n",
    "    labels : list\n",
    "        List of MNE label objects for brain regions\n",
    "    view_angles : list, optional\n",
    "        List of view angles for brain visualization\n",
    "    subjects_dir : str, optional\n",
    "        Directory containing FreeSurfer subject data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The figure containing all brain visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_centers = kmeans_results['C']\n",
    "    n_clusters = cluster_centers.shape[0]\n",
    "    \n",
    "    # Create a custom colormap with distinct vibrant colors (no greys or whites)\n",
    "    colors = [\n",
    "        '#e41a1c',  # red\n",
    "        '#377eb8',  # blue\n",
    "        '#4daf4a',  # green\n",
    "        '#984ea3',  # purple\n",
    "        '#ff7f00',  # orange\n",
    "        '#ffff33',  # yellow\n",
    "        '#a65628',  # brown\n",
    "        '#f781bf',  # pink\n",
    "        '#1b9e77',  # teal\n",
    "        '#d95f02',  # vermillion\n",
    "        '#7570b3',  # slate blue\n",
    "        '#e6ab02',  # mustard\n",
    "    ]\n",
    "    # If more than 12 clusters, we'll cycle through these colors\n",
    "    custom_cmap = LinearSegmentedColormap.from_list('custom_vibrant', colors[:n_clusters], N=n_clusters)\n",
    "    \n",
    "    # Prepare subplot grid\n",
    "    fig, axes = plt.subplots(n_clusters, len(view_angles),\n",
    "                           figsize=(2.5 * len(view_angles), 2 * n_clusters))\n",
    "    \n",
    "    for c in range(n_clusters):\n",
    "        center_vec = cluster_centers[c, :]\n",
    "        pos_idx = np.where(center_vec > 0)[0]\n",
    "        print(f\"Cluster {c}: {len(pos_idx)} minority ROIs\")\n",
    "    \n",
    "        # Create a fresh Brain for this cluster\n",
    "        Brain = mne.viz.get_brain_class()\n",
    "        brain = Brain(\n",
    "            subject=\"fsaverage\",\n",
    "            hemi=\"both\",\n",
    "            surf=\"pial\",\n",
    "            subjects_dir=subjects_dir,\n",
    "            background=\"white\",\n",
    "            size=(400, 400),\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "        # Highlight only the positive ROIs\n",
    "        color = colors[c % len(colors)]  # Use our vibrant colors\n",
    "        for idx in pos_idx:\n",
    "            lab = labels[idx]\n",
    "            brain.add_label(lab, color=color, alpha=1.0, borders=False)\n",
    "            brain.add_label(lab, color='black', alpha=0.8, borders=True)\n",
    "    \n",
    "        # Loop over each view, capture screenshot, and plot into the grid\n",
    "        for j, view in enumerate(view_angles):\n",
    "            if view == 'lateral-left':\n",
    "                brain.show_view('lateral', hemi='lh')\n",
    "            elif view == 'lateral-right':\n",
    "                brain.show_view('lateral', hemi='rh')\n",
    "            else:\n",
    "                brain.show_view(view)\n",
    "            \n",
    "            img = brain.screenshot()\n",
    "            ax = axes[c, j] if n_clusters > 1 else axes[j]\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add column headers (view names) to the top row\n",
    "            if c == 0:\n",
    "                clean_name = view.replace('-', ' ').capitalize()\n",
    "                ax.set_title(clean_name, fontsize=12)\n",
    "            \n",
    "            # Add row labels (cluster numbers) to the leftmost column\n",
    "            if j == 0:\n",
    "                # Create more prominent cluster labels with boxes\n",
    "                ax.text(-0.15, 0.5, f\"Cluster {c}\", fontsize=12, \n",
    "                       ha='right', va='center', transform=ax.transAxes,\n",
    "                       bbox=dict(facecolor=color, alpha=0.5, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "        brain.close()\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(f\"LEiDA States (K={k})\", fontsize=16, y=1.02)\n",
    "    \n",
    "    # Adjust layout to make room for the row labels\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0.1, wspace=0.05)\n",
    "    \n",
    "    # Add description at the bottom\n",
    "    plt.figtext(0.5, -0.05, \n",
    "               f\"Brain regions showing opposite-phase activity within each state.\\n\"\n",
    "               f\"Each row represents one of the {n_clusters} states identified by k-means clustering.\",\n",
    "               ha='center', fontsize=10)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "k = 10\n",
    "roi_names = epochs.ch_names  # Each name is the label of a parcellated ROI\n",
    "print(\"ROI names:\", roi_names)\n",
    "\n",
    "# Read Desikan–Killiany annotation from fsaverage\n",
    "SUBJECTS_DIR = \"~/mne_data/MNE-fsaverage-data/\" \n",
    "labels = mne.read_labels_from_annot(\n",
    "    subject=\"fsaverage\",\n",
    "    parc=\"aparc\",\n",
    "    subjects_dir=SUBJECTS_DIR\n",
    ")\n",
    "# remove \"unknown\" if needed\n",
    "if labels and labels[-1].name == \"unknown\":\n",
    "    labels = labels[:-1]\n",
    "\n",
    "view_angles = ['lateral-left', 'lateral-right', 'dorsal', 'ventral']\n",
    "fig = visualize_brain_clusters(k, kmeans_results, roi_names, labels, view_angles=view_angles)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Occurrence probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "chosen_k = 10\n",
    "solution = kmeans_results\n",
    "all_labels = solution['IDX']\n",
    "time_points = len(all_labels)\n",
    "\n",
    "# Compute occurrence probability for each cluster\n",
    "prob_vec = [(all_labels == c).mean() for c in range(chosen_k)]\n",
    "\n",
    "palette = sns.color_palette(\"husl\", chosen_k)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(range(chosen_k), prob_vec, color=palette)\n",
    "plt.xlabel(\"Cluster (State)\")\n",
    "plt.ylabel(\"Occurrence Probability\")\n",
    "plt.title(f\"State Occurrence Probability (Single Subject, K={chosen_k})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "chosen_k = 10\n",
    "solution = kmeans_results\n",
    "palette = sns.color_palette(\"husl\", chosen_k)\n",
    "\n",
    "# 'labels_dict' contains labels per condition and subject.\n",
    "conditions = [\"Coordination\", \"Solo\", \"Spontaneous\"]\n",
    "\n",
    "# Calculate the occurrence probability for each condition separately\n",
    "condition_probs = {}\n",
    "for condition in conditions:\n",
    "    labels_list = []\n",
    "    for subject, labels in labels_dict[condition].items():\n",
    "        labels_list.append(labels.flatten())\n",
    "    labels_concat = np.hstack(labels_list)\n",
    "    prob_vec = [(labels_concat == c).mean() for c in range(chosen_k)]\n",
    "    condition_probs[condition] = prob_vec\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 4), sharey=True)\n",
    "\n",
    "for idx, condition in enumerate(conditions):\n",
    "    ax = axes[idx]\n",
    "    prob_vec = condition_probs[condition]\n",
    "    ax.bar(range(chosen_k), prob_vec, color=palette)\n",
    "    ax.set_title(f\"{condition}\", fontsize=14)\n",
    "    ax.set_xlabel(\"Cluster (State)\", fontsize=12)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel(\"Occurrence Probability\", fontsize=12)\n",
    "    ax.set_xticks(range(chosen_k))\n",
    "    ax.set_ylim(0, max(max(p) for p in condition_probs.values()) * 1.1)  # consistent y-axis\n",
    "\n",
    "fig.suptitle(f\"State Occurrence Probability per Condition (K={chosen_k})\", fontsize=16, y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "conditions = [\"Coordination\", \"Solo\", \"Spontaneous\"]\n",
    "all_subjects = set()  # gather from your data\n",
    "\n",
    "# Suppose we keep a nested structure: occ_prob[condition] = 2D array (n_subjects x K)\n",
    "occ_prob = {cond: None for cond in conditions}\n",
    "\n",
    "# 1) Build a consistent list of subjects, to ensure correct row ordering\n",
    "for cond in conditions:\n",
    "    all_subjects.update(labels_dict[cond].keys())\n",
    "all_subjects = sorted(all_subjects)\n",
    "n_subj = len(all_subjects)\n",
    "\n",
    "# 2) Fill in the per-subject cluster probabilities\n",
    "for cond in conditions:\n",
    "    # array shape = (n_subj, K)\n",
    "    arr = np.zeros((n_subj, k), dtype=np.float64)\n",
    "    for si, subject in enumerate(all_subjects):\n",
    "        if subject not in labels_dict[cond]:\n",
    "            # e.g., if subject doesn't exist for that condition\n",
    "            arr[si, :] = np.nan\n",
    "            print(f\"Warning: subject {subject} not found for condition {cond}. Filling with NaN.\")\n",
    "            continue\n",
    "        \n",
    "        labels_2d = labels_dict[cond][subject]  # shape (n_epochs, n_windows)\n",
    "        all_labels_1d = labels_2d.flatten()      # shape (n_total_timepoints,)\n",
    "\n",
    "        for c in range(k):\n",
    "            arr[si, c] = np.mean(all_labels_1d == c)\n",
    "    occ_prob[cond] = arr\n",
    "\n",
    "print(f\"Number of probability values per condition: {len(occ_prob[conditions[0]])}\")\n",
    "print(f\"Shape of probability array for {conditions[0]}: {occ_prob[conditions[0]].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Data preparation (already prepared by you)\n",
    "# data_list = []\n",
    "# for c in range(k):\n",
    "#     for cond in conditions:\n",
    "#         for subj_idx, subj in enumerate(all_subjects):\n",
    "#             prob = occ_prob[cond][subj_idx, c]\n",
    "#             if not np.isnan(prob):\n",
    "#                 data_list.append({\n",
    "#                     \"Subject\": subj,\n",
    "#                     \"Condition\": cond,\n",
    "#                     \"Cluster\": f\"Cluster {c}\",\n",
    "#                     \"Probability\": prob\n",
    "#                 })\n",
    "\n",
    "# df = pd.DataFrame(data_list)\n",
    "\n",
    "# # Define colors for conditions\n",
    "# condition_colors = {\n",
    "#     'Coordination': 'orchid',\n",
    "#     'Solo': 'yellowgreen',\n",
    "#     'Spontaneous': 'lightblue'\n",
    "# }\n",
    "\n",
    "# # Plot vertical raincloud plot per cluster\n",
    "# for cluster in df['Cluster'].unique():\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     cluster_df = df[df['Cluster'] == cluster]\n",
    "\n",
    "#     conditions_order = [\"Coordination\", \"Solo\", \"Spontaneous\"]\n",
    "\n",
    "#     for idx, cond in enumerate(conditions_order):\n",
    "#         data_cond = cluster_df[cluster_df[\"Condition\"] == cond][\"Probability\"].values\n",
    "\n",
    "#         # Violin plot (density distribution)\n",
    "#         vp = plt.violinplot(data_cond, positions=[idx], points=300,\n",
    "#                             vert=True, widths=0.7,\n",
    "#                             showmeans=False, showmedians=False, showextrema=False)\n",
    "\n",
    "#         # Cut violin plot in half (right side only)\n",
    "#         for b in vp['bodies']:\n",
    "#             path = b.get_paths()[0].vertices\n",
    "#             mean_x = np.mean(path[:, 0])\n",
    "#             path[:, 0] = np.clip(path[:, 0], idx, idx + 0.4)\n",
    "#             b.set_facecolor(condition_colors[cond])\n",
    "#             b.set_alpha(0.5)\n",
    "#             b.set_edgecolor('none')\n",
    "\n",
    "#         # Boxplot\n",
    "#         bp = plt.boxplot(data_cond, positions=[idx - 0.15], widths=0.1,\n",
    "#                          patch_artist=True, vert=True,\n",
    "#                          showcaps=False, boxprops=dict(facecolor=condition_colors[cond], alpha=0.7),\n",
    "#                          medianprops=dict(color=\"k\", linewidth=1.5),\n",
    "#                          whiskerprops=dict(color=condition_colors[cond], linewidth=1.5),\n",
    "#                          flierprops=dict(marker='o', color='gray', alpha=0.5))\n",
    "\n",
    "#         # Scatter plot (rain drops)\n",
    "#         x_jittered = np.random.uniform(idx - 0.35, idx - 0.2, size=len(data_cond))\n",
    "#         plt.scatter(x_jittered, data_cond, color=condition_colors[cond], alpha=0.6, s=15)\n",
    "\n",
    "#     plt.xticks(range(len(conditions_order)), conditions_order, fontsize=12)\n",
    "#     plt.ylabel(\"Occurrence Probability\", fontsize=12)\n",
    "#     plt.title(f\"Raincloud Plot for {cluster}\", fontsize=14)\n",
    "#     sns.despine(trim=True)\n",
    "#     plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "\n",
    "# Define your condition pairs clearly\n",
    "condition_pairs = [\n",
    "    (\"Coordination\", \"Solo\"),\n",
    "    (\"Coordination\", \"Spontaneous\"),\n",
    "    (\"Solo\", \"Spontaneous\")\n",
    "]\n",
    "\n",
    "alpha = 0.05\n",
    "k = occ_prob['Coordination'].shape[1]  # number of clusters\n",
    "\n",
    "# Run paired t-tests for each pair of conditions and each cluster\n",
    "for cond1, cond2 in condition_pairs:\n",
    "    print(f\"\\nPaired t-test between '{cond1}' and '{cond2}':\")\n",
    "    p_values = []\n",
    "\n",
    "    for c in range(k):\n",
    "        # Get paired data for the current cluster\n",
    "        data1 = occ_prob[cond1][:, c]\n",
    "        data2 = occ_prob[cond2][:, c]\n",
    "\n",
    "        # Only consider subjects without missing data\n",
    "        mask = ~np.isnan(data1) & ~np.isnan(data2)\n",
    "        data1_clean, data2_clean = data1[mask], data2[mask]\n",
    "\n",
    "        # Perform paired t-test\n",
    "        t_stat, p_val = ttest_rel(data1_clean, data2_clean)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    # Multiple-comparison correction (e.g., Bonferroni or FDR)\n",
    "    from statsmodels.stats.multitest import fdrcorrection\n",
    "    rejected, p_values_corrected = fdrcorrection(p_values, alpha=alpha)\n",
    "\n",
    "    # Print results per cluster\n",
    "    for c, (p_uncorrected, p_corr, reject) in enumerate(zip(p_values, p_values_corrected, rejected)):\n",
    "        sig_str = \"✅ significant\" if reject else \"❌ not significant\"\n",
    "        print(f\"  Cluster {c}: p={p_uncorrected:.4f}, corrected p={p_corr:.4f} → {sig_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "# Data preparation (already prepared by you)\n",
    "data_list = []\n",
    "for c in range(k):\n",
    "    for cond in conditions:\n",
    "        for subj_idx, subj in enumerate(all_subjects):\n",
    "            prob = occ_prob[cond][subj_idx, c]\n",
    "            if not np.isnan(prob):\n",
    "                data_list.append({\n",
    "                    \"Subject\": subj,\n",
    "                    \"Condition\": cond,\n",
    "                    \"Cluster\": f\"Cluster {c}\",\n",
    "                    \"Probability\": prob\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Colors for conditions\n",
    "condition_colors = {\n",
    "    'Coordination': 'orchid',\n",
    "    'Solo': 'yellowgreen',\n",
    "    'Spontaneous': 'lightblue'\n",
    "}\n",
    "\n",
    "condition_pairs = [(\"Coordination\", \"Solo\"),\n",
    "                   (\"Coordination\", \"Spontaneous\"),\n",
    "                   (\"Solo\", \"Spontaneous\")]\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "# Run paired t-tests and store significant results\n",
    "significance_results = {}\n",
    "for cond1, cond2 in condition_pairs:\n",
    "    sig_list = []\n",
    "    p_values = []\n",
    "\n",
    "    for c in range(k):\n",
    "        data1 = occ_prob[cond1][:, c]\n",
    "        data2 = occ_prob[cond2][:, c]\n",
    "\n",
    "        mask = ~np.isnan(data1) & ~np.isnan(data2)\n",
    "        data1_clean, data2_clean = data1[mask], data2[mask]\n",
    "\n",
    "        t_stat, p_val = ttest_rel(data1_clean, data2_clean)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    # FDR correction\n",
    "    rejected, p_values_corrected = fdrcorrection(p_values, alpha=alpha)\n",
    "    significance_results[(cond1, cond2)] = rejected\n",
    "\n",
    "# Plotting raincloud plots with significance bars\n",
    "for cluster_idx, cluster in enumerate(df['Cluster'].unique()):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cluster_df = df[df['Cluster'] == cluster]\n",
    "\n",
    "    conditions_order = [\"Coordination\", \"Solo\", \"Spontaneous\"]\n",
    "\n",
    "    positions = np.arange(len(conditions_order))\n",
    "\n",
    "    for idx, cond in enumerate(conditions_order):\n",
    "        data_cond = cluster_df[cluster_df[\"Condition\"] == cond][\"Probability\"].values\n",
    "\n",
    "        # Violin plot (half)\n",
    "        vp = plt.violinplot(data_cond, positions=[positions[idx]], points=300,\n",
    "                            vert=True, widths=0.7,\n",
    "                            showmeans=False, showmedians=False, showextrema=False)\n",
    "\n",
    "        for b in vp['bodies']:\n",
    "            path = b.get_paths()[0].vertices\n",
    "            path[:, 0] = np.clip(path[:, 0], positions[idx], positions[idx] + 0.4)\n",
    "            b.set_facecolor(condition_colors[cond])\n",
    "            b.set_alpha(0.5)\n",
    "            b.set_edgecolor('none')\n",
    "\n",
    "        # Boxplot\n",
    "        plt.boxplot(data_cond, positions=[positions[idx] - 0.15], widths=0.1,\n",
    "                    patch_artist=True, vert=True,\n",
    "                    showcaps=False, boxprops=dict(facecolor=condition_colors[cond], alpha=0.7),\n",
    "                    medianprops=dict(color=\"k\", linewidth=1.5),\n",
    "                    whiskerprops=dict(color=condition_colors[cond], linewidth=1.5),\n",
    "                    flierprops=dict(marker='o', color='gray', alpha=0.5))\n",
    "\n",
    "        # Scatter plot (rain drops)\n",
    "        x_jittered = np.random.uniform(positions[idx] - 0.35, positions[idx] - 0.2, size=len(data_cond))\n",
    "        plt.scatter(x_jittered, data_cond, color='black', alpha=0.6, s=15)\n",
    "\n",
    "    plt.xticks(positions, conditions_order, fontsize=12)\n",
    "    plt.ylabel(\"Occurrence Probability\", fontsize=12)\n",
    "    plt.title(f\"Raincloud Plot for {cluster}\", fontsize=14)\n",
    "    sns.despine(trim=True)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add significance bars and asterisks\n",
    "    ymax = cluster_df[\"Probability\"].max()\n",
    "    ystart = ymax + 0.03  # start a bit above the top data point\n",
    "    ystep = 0.02  # space between bars\n",
    "\n",
    "    pair_offsets = {(\"Coordination\", \"Solo\"): (-0.1, 0.1),\n",
    "                    (\"Coordination\", \"Spontaneous\"): (-0.1, 0.2),\n",
    "                    (\"Solo\", \"Spontaneous\"): (0.1, 0.2)}\n",
    "\n",
    "    for (cond1, cond2), significant_array in significance_results.items():\n",
    "        if significant_array[cluster_idx]:\n",
    "            idx1 = conditions_order.index(cond1)\n",
    "            idx2 = conditions_order.index(cond2)\n",
    "            y = ystart\n",
    "            ystart += ystep  # increment for next bar\n",
    "\n",
    "            # Draw the significance bar\n",
    "            plt.plot([positions[idx1], positions[idx1], positions[idx2], positions[idx2]],\n",
    "                     [y, y + ystep, y + ystep, y], lw=1.5, color='black')\n",
    "\n",
    "            # Add asterisk at midpoint\n",
    "            mid = (positions[idx1] + positions[idx2]) / 2\n",
    "            plt.text(mid, y + ystep, \"*\", ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "    plt.ylim(0, ystart + 0.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "# Conditions and pairs\n",
    "conditions = [\"Coordination\", \"Solo\", \"Spontaneous\"]\n",
    "condition_pairs = [\n",
    "    (\"Coordination\", \"Solo\"),\n",
    "    (\"Coordination\", \"Spontaneous\"),\n",
    "    (\"Solo\", \"Spontaneous\")\n",
    "]\n",
    "\n",
    "# Compute paired t-tests\n",
    "alpha = 0.05\n",
    "k = occ_prob['Coordination'].shape[1]\n",
    "\n",
    "# Store all results clearly\n",
    "test_results = {}  # key: (cond1, cond2), value: list of (t_stat, p_value, corrected_p, significant)\n",
    "\n",
    "for cond1, cond2 in condition_pairs:\n",
    "    p_values = []\n",
    "    t_stats = []\n",
    "    \n",
    "    for c in range(k):\n",
    "        data1 = occ_prob[cond1][:, c]\n",
    "        data2 = occ_prob[cond2][:, c]\n",
    "\n",
    "        mask = ~np.isnan(data1) & ~np.isnan(data2)\n",
    "        data1_clean, data2_clean = data1[mask], data2[mask]\n",
    "\n",
    "        t_stat, p_val = ttest_rel(data1_clean, data2_clean)\n",
    "        p_values.append(p_val)\n",
    "        t_stats.append(t_stat)\n",
    "\n",
    "    rejected, p_corrected = fdrcorrection(p_values, alpha=alpha)\n",
    "\n",
    "    test_results[(cond1, cond2)] = [\n",
    "        (t_stats[c], p_values[c], p_corrected[c], rejected[c]) for c in range(k)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data into a DataFrame\n",
    "data_list = []\n",
    "for c in range(k):\n",
    "    for cond in conditions:\n",
    "        for subj_idx, subj in enumerate(all_subjects):\n",
    "            prob = occ_prob[cond][subj_idx, c]\n",
    "            if not np.isnan(prob):\n",
    "                data_list.append({\n",
    "                    \"Subject\": subj,\n",
    "                    \"Condition\": cond,\n",
    "                    \"Cluster\": f\"Cluster {c}\",\n",
    "                    \"Probability\": prob\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Colors for conditions\n",
    "condition_colors = {'Coordination':'orchid','Solo':'yellowgreen','Spontaneous':'lightblue'}\n",
    "\n",
    "# Plot vertical raincloud plots\n",
    "for c in range(k):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    cluster_df = df[df['Cluster'] == f'Cluster {c}']\n",
    "\n",
    "    positions = np.arange(len(conditions))\n",
    "    for idx, cond in enumerate(conditions):\n",
    "        data_cond = cluster_df[cluster_df[\"Condition\"] == cond][\"Probability\"].values\n",
    "\n",
    "        # Violin plot (density)\n",
    "        vp = plt.violinplot(data_cond, positions=[idx], points=300, vert=True, widths=0.7,\n",
    "                            showmeans=False, showmedians=False, showextrema=False)\n",
    "        for b in vp['bodies']:\n",
    "            path = b.get_paths()[0].vertices\n",
    "            path[:,0] = np.clip(path[:,0], idx, idx+0.4)\n",
    "            b.set_facecolor(condition_colors[cond])\n",
    "            b.set_alpha(0.5)\n",
    "            b.set_edgecolor('none')\n",
    "\n",
    "        # Boxplot\n",
    "        plt.boxplot(data_cond, positions=[idx-0.15], widths=0.1, vert=True,\n",
    "                    showcaps=False, patch_artist=True,\n",
    "                    boxprops=dict(facecolor=condition_colors[cond],alpha=0.7),\n",
    "                    medianprops=dict(color='k'))\n",
    "\n",
    "        # Scatter points\n",
    "        jitter = np.random.uniform(idx-0.35, idx-0.2, size=len(data_cond))\n",
    "        plt.scatter(jitter, data_cond, color='black', alpha=0.6, s=15)\n",
    "\n",
    "    plt.xticks(positions, conditions, fontsize=12)\n",
    "    plt.ylabel(\"Occurrence Probability\", fontsize=12)\n",
    "    plt.title(f\"Raincloud Plot for Cluster {c}\", fontsize=14)\n",
    "    sns.despine(trim=True)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add significance annotations\n",
    "    y_max = cluster_df['Probability'].max()\n",
    "    y_start = y_max + 0.02\n",
    "    y_step = (y_max * 0.05)\n",
    "\n",
    "    for idx, (cond1, cond2) in enumerate(condition_pairs):\n",
    "        t_stat, p_val, p_corr, significant = test_results[(cond1, cond2)][c]\n",
    "\n",
    "        # Select positions\n",
    "        x1, x2 = positions[conditions.index(cond1)], positions[conditions.index(cond2)]\n",
    "        y = y_start + idx * y_step\n",
    "\n",
    "        # Significance stars\n",
    "        if p_corr < 0.001:\n",
    "            stars = '***'\n",
    "        elif p_corr < 0.01:\n",
    "            stars = '**'\n",
    "        elif p_corr < 0.05:\n",
    "            stars = '*'\n",
    "        else:\n",
    "            stars = 'ns'\n",
    "\n",
    "        # Draw lines for significance\n",
    "        if significant:\n",
    "            plt.plot([x1, x1, x2, x2], [y, y+y_step/4, y+y_step/4, y], lw=1.5, color='grey', alpha=0.7)\n",
    "            plt.text((x1+x2)/2, y+y_step/4, f\"{stars}\",\n",
    "                     ha='center', va='bottom', fontsize=20, color='r')\n",
    "            plt.text((x1+x2)/2, y+y_step/4, f\"p = {p_corr:.5f}\",\n",
    "                     ha='center', va='bottom', fontsize=8, color='grey')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dwell Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_dwell_time_single_subject(labels, chosen_k):\n",
    "    \"\"\"\n",
    "    For each state, compute average dwell time for the single subject:\n",
    "    (total # of windows in that state) / (# of runs).\n",
    "    \"\"\"\n",
    "    dwell_times = np.zeros(chosen_k)\n",
    "    for state in range(chosen_k):\n",
    "        idx = np.where(labels == state)[0]\n",
    "        if len(idx) == 0:\n",
    "            dwell_times[state] = 0\n",
    "            continue\n",
    "        # Number of consecutive runs\n",
    "        diffs = np.diff(idx)\n",
    "        run_breaks = np.where(diffs > 1)[0]  \n",
    "        num_runs = len(run_breaks) + 1\n",
    "        dwell_times[state] = len(idx) / num_runs\n",
    "    return dwell_times\n",
    "\n",
    "# Example usage:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "solution = kmeans_results\n",
    "all_labels = solution['IDX']\n",
    "\n",
    "dwell_times = compute_dwell_time_single_subject(all_labels, k)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=list(range(k)), y=dwell_times)\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Dwell Time (avg. consecutive windows in state)\")\n",
    "plt.title(f\"Dwell Time per State (Single Subject, K={chosen_k})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "# Conditions and pairs\n",
    "conditions = [\"Coordination\", \"Solo\", \"Spontaneous\"]\n",
    "condition_pairs = [\n",
    "    (\"Coordination\", \"Solo\"),\n",
    "    (\"Coordination\", \"Spontaneous\"),\n",
    "    (\"Solo\", \"Spontaneous\")\n",
    "]\n",
    "\n",
    "alpha = 0.05\n",
    "k = dwell_time_dict['Coordination'].shape[1]\n",
    "\n",
    "# Perform paired t-tests across conditions per cluster\n",
    "test_results = {}\n",
    "\n",
    "for cond1, cond2 in condition_pairs:\n",
    "    p_values = []\n",
    "    for state in range(k):\n",
    "        data1 = dwell_time_dict[cond1][:, state]\n",
    "        data2 = dwell_time_dict[cond2][:, state]\n",
    "\n",
    "        mask = ~np.isnan(data1) & ~np.isnan(data2)\n",
    "        t_stat, p_val = ttest_rel(data1[mask], data2[mask])\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    # Multiple comparisons correction\n",
    "    rejected, p_corrected = fdrcorrection(p_values, alpha=alpha)\n",
    "    test_results[(cond1, cond2)] = [(p_values[i], p_corrected[i], rejected[i]) for i in range(k)]\n",
    "\n",
    "# Plot boxplots with significance bars per cluster\n",
    "for state in range(k):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Prepare data\n",
    "    dwell_data = []\n",
    "    for cond in conditions:\n",
    "        for subj_dwell in dwell_time_dict[cond]:\n",
    "            dwell_data.append({\n",
    "                \"Condition\": cond, \n",
    "                \"Dwell_Time\": subj_dwell[state]\n",
    "            })\n",
    "    df_dwell = pd.DataFrame(dwell_data)\n",
    "\n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df_dwell, x=\"Condition\", y=\"Dwell_Time\",\n",
    "                palette=\"Set2\", linewidth=1.5)\n",
    "    \n",
    "    # Overlay individual points\n",
    "    sns.stripplot(data=df_dwell, x=\"Condition\", y=\"Dwell_Time\",\n",
    "                  color=\"black\", alpha=0.6, jitter=True)\n",
    "\n",
    "    plt.title(f\"Dwell Time Distribution for Cluster {state}\", fontsize=14)\n",
    "    plt.ylabel(\"Average Dwell Time (windows)\")\n",
    "    plt.xlabel(\"Condition\")\n",
    "    sns.despine(trim=True)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Add significance bars\n",
    "    y_max = df_dwell[\"Dwell_Time\"].max()\n",
    "    y_start = y_max + y_max*0.05\n",
    "    y_step = y_max * 0.05\n",
    "\n",
    "    pos_dict = {cond: idx for idx, cond in enumerate(conditions)}\n",
    "\n",
    "    for idx, (cond1, cond2) in enumerate(condition_pairs):\n",
    "        p_val, p_corr, reject = test_results[(cond1, cond2)][state]\n",
    "\n",
    "        x1, x2 = pos_dict[cond1], pos_dict[cond2]\n",
    "        y = y_start + idx * y_step\n",
    "\n",
    "        # Significance stars\n",
    "        if p_corr < 0.001:\n",
    "            stars = '***'\n",
    "        elif p_corr < 0.01:\n",
    "            stars = '**'\n",
    "        elif p_corr < 0.05:\n",
    "            stars = '*'\n",
    "        else:\n",
    "            stars = 'ns'\n",
    "\n",
    "        # Draw line and annotation\n",
    "        plt.plot([x1, x1, x2, x2], [y, y+y_step/4, y+y_step/4, y], lw=1.5, color='k')\n",
    "        plt.text((x1+x2)/2, y+y_step/4, f\"{stars}\\n(p={p_corr:.3f})\",\n",
    "                 ha='center', va='bottom', fontsize=10, color='k')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_transition_matrix_single_subject(labels, chosen_k):\n",
    "    \"\"\"\n",
    "    Returns a chosen_k x chosen_k matrix, where entry (i, j)\n",
    "    is the probability of transitioning from state i to j.\n",
    "    \"\"\"\n",
    "    transition_mat = np.zeros((chosen_k, chosen_k))\n",
    "    for t in range(len(labels) - 1):\n",
    "        from_state = labels[t]\n",
    "        to_state = labels[t + 1]\n",
    "        transition_mat[from_state, to_state] += 1\n",
    "\n",
    "    row_sums = transition_mat.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    transition_mat /= row_sums\n",
    "    return transition_mat\n",
    "\n",
    "# Example usage + plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "solution = kmeans_results\n",
    "all_labels = solution['IDX']\n",
    "\n",
    "transition_matrix = compute_transition_matrix_single_subject(all_labels, k)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(transition_matrix, annot=True, cmap=\"Blues\", linewidths=0.5, fmt=\".2f\")\n",
    "plt.xlabel(\"To State\")\n",
    "plt.ylabel(\"From State\")\n",
    "plt.title(f\"State Transition Matrix (Single Subject, K={chosen_k})\")\n",
    "plt.xticks(ticks=np.arange(chosen_k)+0.5, labels=[str(i) for i in range(chosen_k)])\n",
    "plt.yticks(ticks=np.arange(chosen_k)+0.5, labels=[str(i) for i in range(chosen_k)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_transition_matrix(labels_epoch, chosen_k):\n",
    "    \"\"\"\n",
    "    Compute transition matrix for a single epoch.\n",
    "    Returns a chosen_k x chosen_k transition matrix.\n",
    "    \"\"\"\n",
    "    transition_mat = np.zeros((chosen_k, chosen_k))\n",
    "    for t in range(len(labels_epoch) - 1):\n",
    "        from_state = labels_epoch[t]\n",
    "        to_state = labels_epoch[t + 1]\n",
    "        transition_mat[from_state, to_state] += 1\n",
    "\n",
    "    # Normalize rows to get transition probabilities\n",
    "    row_sums = transition_mat.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  # Prevent division by zero\n",
    "    transition_mat /= row_sums\n",
    "    return transition_mat\n",
    "\n",
    "# Now clearly average across epochs and subjects:\n",
    "conditions = [\"Coordination\", \"Solo\", \"Spontaneous\"]\n",
    "\n",
    "transition_matrices = {}\n",
    "\n",
    "for cond in conditions:\n",
    "    subject_matrices = []\n",
    "    for subj in labels_dict[cond]:\n",
    "        epochs_labels = labels_dict[cond][subj]  # shape: (n_epochs, n_windows)\n",
    "        epoch_matrices = []\n",
    "        for epoch_labels in epochs_labels:\n",
    "            epoch_trans_mat = compute_transition_matrix(epoch_labels, k)\n",
    "            epoch_matrices.append(epoch_trans_mat)\n",
    "        # Average transition matrices across epochs for the current subject\n",
    "        subj_mean_matrix = np.mean(epoch_matrices, axis=0)\n",
    "        subject_matrices.append(subj_mean_matrix)\n",
    "    # Average transition matrices across subjects for this condition\n",
    "    condition_mean_matrix = np.mean(subject_matrices, axis=0)\n",
    "    transition_matrices[cond] = condition_mean_matrix\n",
    "\n",
    "# Plotting clearly with heatmaps:\n",
    "for cond in conditions:\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.heatmap(transition_matrices[cond], annot=True, cmap=\"Blues\",\n",
    "                linewidths=0.5, fmt=\".2f\", square=True,\n",
    "                xticklabels=[f\"{i}\" for i in range(k)],\n",
    "                yticklabels=[f\"{i}\" for i in range(k)])\n",
    "    plt.title(f\"Average State Transition Matrix ({cond})\", fontsize=14)\n",
    "    plt.xlabel(\"To State\")\n",
    "    plt.ylabel(\"From State\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "condition_pairs = [\n",
    "    (\"Coordination\", \"Solo\"),\n",
    "    (\"Coordination\", \"Spontaneous\"),\n",
    "    (\"Solo\", \"Spontaneous\")\n",
    "]\n",
    "\n",
    "# Step 1: Compute all difference matrices and find max abs value\n",
    "diff_matrices = {}\n",
    "all_diffs = []\n",
    "\n",
    "for cond1, cond2 in condition_pairs:\n",
    "    diff = transition_matrices[cond1] - transition_matrices[cond2]\n",
    "    diff_matrices[(cond1, cond2)] = diff\n",
    "    all_diffs.append(np.abs(diff))\n",
    "\n",
    "# Global symmetric colorbar limits\n",
    "vmax = np.max(all_diffs)\n",
    "vmin = -vmax\n",
    "\n",
    "# Step 2: Plot each with same color scale\n",
    "for (cond1, cond2), diff_matrix in diff_matrices.items():\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(diff_matrix, annot=True, cmap=\"bwr\", center=0, linewidths=0.5, fmt=\".2f\",\n",
    "                vmin=vmin, vmax=vmax,\n",
    "                xticklabels=[f\"{i}\" for i in range(k)],\n",
    "                yticklabels=[f\"{i}\" for i in range(k)])\n",
    "    plt.title(f\"Transition Matrix Difference: {cond1} − {cond2}\", fontsize=14)\n",
    "    plt.xlabel(\"To State\")\n",
    "    plt.ylabel(\"From State\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirror",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
